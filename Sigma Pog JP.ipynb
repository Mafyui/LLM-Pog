{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8660db51-f114-43ee-9b79-7204f719c587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['output', 'input', 'instruction'],\n",
      "        num_rows: 51760\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from Single_Block_Fortnite import EmbeddingNN, AttentionNN, NormNN, FFN\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "from datasets import load_dataset\n",
    "\n",
    "# This should work now since you are logged in\n",
    "\n",
    "\n",
    "ds = load_dataset(\"yahma/alpaca-cleaned\")\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ec546ff-473e-447e-8c9f-392ef59e75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Vocab size: 50257\n",
      "Data shape: torch.Size([9731408])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import tiktoken\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load dataset (assuming already logged in via Hugging Face)\n",
    "\n",
    "train_texts = ds['train']  # list of all text entries in train split\n",
    "\n",
    "# Initialize GPT-2 tokenizer\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "vocab_size = tokenizer.n_vocab\n",
    "print(\"Vocab size:\", vocab_size)\n",
    "\n",
    "# Encode all text entries into a single list of integers\n",
    "integers = []\n",
    "end_token_id = 50256  # GPT-2 end token\n",
    "\n",
    "for text in train_texts:\n",
    "    tokens = tokenizer.encode(str(text))\n",
    "    integers.extend(tokens)     # add tokens\n",
    "    integers.append(end_token_id)  # append end-of-text token\n",
    "\n",
    "# Convert to PyTorch tensor\n",
    "data = torch.tensor(integers, dtype=torch.long).to(device)\n",
    "print(\"Data shape:\", data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b796f9a1-60aa-4e45-a807-56cba8d48e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#text = ds\n",
    "#with open(\"C:/Users/PC/Desktop/Important Data/Shakespeare_Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    #text = f.read()\n",
    "  \n",
    "    \n",
    "#integers = tokenizer.encode(str(text)) \n",
    "#vocab_size = tokenizer.n_vocab  # Total number of tokens in the tokenizer\n",
    "#end_token_id = 50256  \n",
    "#integers.append(end_token_id) \n",
    "#data = torch.tensor(integers , dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "#print(vocab_size)\n",
    "#data = torch.tensor(integers , dtype = torch.long)\n",
    "#batch_x = data[i:i+seq_len].unsqueeze(0)\n",
    "   #batch_x = batch_x.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "37099373-797e-4610-b91b-82fb6a6ccdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = AttentionNN(hidden_dim, num_heads)\n",
    "        self.ffn = FFN(hidden_dim)\n",
    "        self.norm1 = NormNN(hidden_dim)\n",
    "        self.norm2 = NormNN(hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attention_output = self.attention(X)\n",
    "        X = X + attention_output\n",
    "\n",
    "        pog_X = self.norm1(X)\n",
    "\n",
    "        pogger_X = self.ffn(pog_X) + pog_X\n",
    "\n",
    "        poggers_X = self.norm2(pogger_X)\n",
    "\n",
    "        return poggers_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4f0abb5-b123-4934-93a6-aa6b3d98582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size , hidden_dim , num_heads,max_sequence_length, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = EmbeddingNN(vocab_size, hidden_dim, max_sequence_length)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(hidden_dim, num_heads) for i in range(N)])\n",
    "        self.linear = nn.Linear(hidden_dim,vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3411ebba-7f2a-479d-b16b-dc92ef20a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "#integers = tokenizer.encode(text)\n",
    "#decode_dict = {i : chars for i , chars in enumerate(chars)}\n",
    "#encode_dict = {chars : i for i , chars in enumerate(chars)}\n",
    "\n",
    "#encode = lambda text : [encode_dict[i] for i in text]\n",
    "#decode = lambda indices : ''.join([decode_dict[i] for i in indices])\n",
    "\n",
    "#encoded_text = encode(text)\n",
    "\n",
    "#print(encode_dict['<END>'])\n",
    "#print(\"Max index in data:\", max(encoded_text), \"Vocab size:\", vocab_size)\n",
    " # usually 50256 for GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1cec522e-f44e-4802-ad93-2e5d2e5478bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =16\n",
    "hidden_dim = 512\n",
    "max_sequence_length = 512\n",
    "num_heads = 8\n",
    "seq_len = 128\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6356431-8af6-4192-9fd7-f5cdf6be7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(logits, batch_y)\n\u001b[0;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 27\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     28\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mbackward(\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28mself\u001b[39m, gradient, retain_graph, create_graph, inputs\u001b[38;5;241m=\u001b[39minputs\n\u001b[0;32m    650\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m _engine_run_backward(\n\u001b[0;32m    354\u001b[0m     tensors,\n\u001b[0;32m    355\u001b[0m     grad_tensors_,\n\u001b[0;32m    356\u001b[0m     retain_graph,\n\u001b[0;32m    357\u001b[0m     create_graph,\n\u001b[0;32m    358\u001b[0m     inputs,\n\u001b[0;32m    359\u001b[0m     allow_unreachable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    360\u001b[0m     accumulate_grad\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    361\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "Transformer_model = Transformer(vocab_size, hidden_dim, num_heads, max_sequence_length, 12).to(device)\n",
    "optimizer = optim.Adam(Transformer_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n = 0\n",
    "loss = None\n",
    "batch_y = []\n",
    "for epoch in range(epochs):\n",
    "    n = n+1\n",
    "    print(f\"Epoch Number: {n}\")\n",
    "    for start in range(0, len(data) - batch_size*seq_len, batch_size*seq_len):\n",
    "    \n",
    "        end = start+batch_size*seq_len\n",
    "        if end + 1 > len(data):\n",
    "            break\n",
    "    \n",
    "        batch_x = data[start : end].view(batch_size, seq_len).to(device)\n",
    "        batch_y = data[start+1 : end+1].view(batch_size, seq_len).to(device)\n",
    "        \n",
    "        logits = Transformer_model(batch_x).to(device)\n",
    "    \n",
    "        logits = logits.view(-1 , vocab_size)\n",
    "        batch_y = batch_y.view(-1)\n",
    "    \n",
    "        loss = criterion(logits, batch_y)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if loss is not None:\n",
    "        print(\"Final loss:\", loss.item())\n",
    "    else:\n",
    "        print(\"No batches were processed. Loss not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3109a766-a275-494c-87f4-5b0697aa84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Transformer_model, \"Sigma_Transformer_full.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bd7f04c-f394-4e52-8462-54b39fead789",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"Sigma_Transformer_full.pth\", map_location=device)\n",
    "model.to(device)\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb273179-342c-4f55-a528-fb433f410add",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = input()\n",
    "input_ids = tokenizer.encode(text)\n",
    "input_text = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "inference = []\n",
    "temperature = 0.7\n",
    "while True:\n",
    "    \n",
    "    logits = Transformer_model(input_text).to(device)\n",
    "    next_token = logits[:,-1 ,:]\n",
    "    probs = torch.softmax(next_token / temperature, dim=-1)\n",
    "    next_token_id = torch.multinomial(probs, 1)\n",
    "    input_text = torch.cat([input_text, next_token_id], dim=1)\n",
    "\n",
    "    inference.append(next_token_id.item())\n",
    "\n",
    "    if next_token_id.item() == 50256 or len(inference) >=  200:\n",
    "        break\n",
    "        \n",
    "print(tokenizer.decode(inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe18da-901d-4335-81a0-de653e17f57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
