{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8660db51-f114-43ee-9b79-7204f719c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from Single_Block_Fortnite import EmbeddingNN, AttentionNN, NormNN, FFN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b796f9a1-60aa-4e45-a807-56cba8d48e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POG\n",
      "['\\n', ' ', '!', '$', '&', \"'\", ',', '-', '.', '3', ':', ';', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '<E>']\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "with open(\"C:/Users/PC/Desktop/Important Data/Shakespeare_Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text = f.read()\n",
    "    chars = sorted(list(set(text)))\n",
    "    chars.append(\"<E>\")\n",
    "    vocab_size = len(chars)\n",
    "  \n",
    "stoi = { ch:i for i,ch in enumerate(chars) }\n",
    "itos = { i:ch for i,ch in enumerate(chars) }\n",
    "encode = lambda s: [stoi[c] for c in s] \n",
    "decode = lambda l: ''.join([itos[i] for i in l])\n",
    "data = torch.tensor(encode(text) , dtype = torch.long).to(device)\n",
    "\n",
    "\n",
    "print(decode(encode(\"POG\")))\n",
    "print(chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37099373-797e-4610-b91b-82fb6a6ccdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = AttentionNN(hidden_dim, num_heads)\n",
    "        self.ffn = FFN(hidden_dim)\n",
    "        self.norm1 = NormNN(hidden_dim)\n",
    "        self.norm2 = NormNN(hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attention_output = self.attention(X)\n",
    "        X = X + attention_output\n",
    "\n",
    "        pog_X = self.norm1(X)\n",
    "\n",
    "        pogger_X = self.ffn(pog_X) + pog_X\n",
    "\n",
    "        poggers_X = self.norm2(pogger_X)\n",
    "\n",
    "        return poggers_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d4f0abb5-b123-4934-93a6-aa6b3d98582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size , hidden_dim , num_heads,max_sequence_length, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = EmbeddingNN(vocab_size, hidden_dim, max_sequence_length)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(hidden_dim, num_heads) for i in range(N)])\n",
    "        self.linear = nn.Linear(hidden_dim,vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cec522e-f44e-4802-ad93-2e5d2e5478bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =16\n",
    "hidden_dim = 512\n",
    "max_sequence_length = 512\n",
    "num_heads = 8\n",
    "seq_len = 128\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6356431-8af6-4192-9fd7-f5cdf6be7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1\n",
      "Final loss: 0.03188561648130417\n",
      "Epoch Number: 2\n",
      "Final loss: 0.02372751757502556\n",
      "Epoch Number: 3\n",
      "Final loss: 0.021613026037812233\n"
     ]
    }
   ],
   "source": [
    "Transformer_model = Transformer(vocab_size, hidden_dim, num_heads, max_sequence_length, 12).to(device)\n",
    "optimizer = optim.Adam(Transformer_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n = 0\n",
    "loss = None\n",
    "batch_y = []\n",
    "for epoch in range(epochs):\n",
    "    n = n+1\n",
    "    print(f\"Epoch Number: {n}\")\n",
    "    for start in range(0, len(data) - batch_size*seq_len, batch_size*seq_len):\n",
    "    \n",
    "        end = start+batch_size*seq_len\n",
    "        if end + 1 > len(data):\n",
    "            break\n",
    "    \n",
    "        batch_x = data[start : end].view(batch_size, seq_len).to(device)\n",
    "        batch_y = data[start+1 : end+1].view(batch_size, seq_len).to(device)\n",
    "        \n",
    "        logits = Transformer_model(batch_x).to(device)\n",
    "    \n",
    "        logits = logits.view(-1 , vocab_size)\n",
    "        batch_y = batch_y.view(-1)\n",
    "    \n",
    "        loss = criterion(logits, batch_y)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    if loss is not None:\n",
    "        print(\"Final loss:\", loss.item())\n",
    "    else:\n",
    "        print(\"No batches were processed. Loss not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3109a766-a275-494c-87f4-5b0697aa84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Transformer_model, \"Sigma_Transformer_full.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1bd7f04c-f394-4e52-8462-54b39fead789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " fortnite\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "enntviveiviinvi; LULULLLULLLLLULUQULEinxcizizUQUCl;'ULULLULLULLILUQULLUCALLULLAULu,quveviz; LULUQUJU\n"
     ]
    }
   ],
   "source": [
    "max_length = 100\n",
    "input_user = encode(input())\n",
    "input_text = torch.tensor([input_user]).to(device)\n",
    "inference = []\n",
    "for _ in range(max_length):\n",
    "    logits = Transformer_model(input_text).to(device)\n",
    "    fortnite = logits[:,-1,:]\n",
    "    probs = torch.softmax(fortnite, dim = -1)\n",
    "    next_token = torch.multinomial(probs, 1)\n",
    "    input_text = torch.cat([input_text, next_token], dim=1)\n",
    "    if next_token == \"<E>\":\n",
    "        break\n",
    "    else:\n",
    "        inference.append(decode(next_token.view(-1).tolist()))\n",
    "        \n",
    "print(''.join(inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e374ab9-b5c5-4fc0-9f35-ab70cd94b7da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
