{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8660db51-f114-43ee-9b79-7204f719c587",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import tiktoken\n",
    "import numpy as np\n",
    "from Single_Block_Fortnite import EmbeddingNN, AttentionNN, NormNN, FFN\n",
    "\n",
    "\n",
    "# For JSONL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6ec546ff-473e-447e-8c9f-392ef59e75f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample entry: {'scene_id': 'rem_if_batch1_001', 'speaker': 'rem', 'text': '……Thank you very much for your concern. But…', 'tone': 'polite', 'emotion': 'guarded', 'subtext': 'Acknowledges kindness but resists being spoiled'}\n",
      "Total tokens: 15902\n",
      "Tensor shape: torch.Size([15902])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# 1. Load dataset\n",
    "import json\n",
    "\n",
    "dataset = []\n",
    "with open(\"C:/Users/PC/Desktop/Important Data/Rem_Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if not line:  # skip blank lines\n",
    "            continue\n",
    "        try:\n",
    "            dataset.append(json.loads(line))\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(\"Skipping bad line:\", line)\n",
    "            print(\"Error:\", e)\n",
    "\n",
    "print(\"Sample entry:\", dataset[0])  # sanity check\n",
    "\n",
    "# 2. Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "# 3. Collect all texts\n",
    "all_texts = [entry[\"text\"] for entry in dataset if \"text\" in entry]\n",
    "\n",
    "\n",
    "# 4. Encode all texts into one long sequence\n",
    "ids = []\n",
    "for t in all_texts:\n",
    "    encoded = tokenizer.encode(t, add_special_tokens=False)\n",
    "    encoded.append(tokenizer.eos_token_id or 50256)  # add EOS after each line\n",
    "    ids.extend(encoded)\n",
    "\n",
    "# 5. Convert to tensor\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "data = torch.tensor(ids, dtype=torch.long, device=device)\n",
    "\n",
    "print(\"Total tokens:\", len(ids))\n",
    "print(\"Tensor shape:\", data.shape)\n",
    "vocab_size = tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b796f9a1-60aa-4e45-a807-56cba8d48e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#text = ds\n",
    "#with open(\"C:/Users/PC/Desktop/Important Data/Shakespeare_Data.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "   # text = f.read()\n",
    "  \n",
    "    \n",
    "#integers = tokenizer.encode(str(text)) \n",
    "#vocab_size = tokenizer.n_vocab  # Total number of tokens in the tokenizer\n",
    "#end_token_id = 50256  \n",
    "#integers.append(end_token_id) \n",
    "#data = torch.tensor(integers , dtype=torch.long).to(device)\n",
    "\n",
    "\n",
    "#print(vocab_size)\n",
    "#data = torch.tensor(integers , dtype = torch.long)\n",
    "#batch_x = data[i:i+seq_len].unsqueeze(0)\n",
    "   #batch_x = batch_x.repeat(batch_size, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37099373-797e-4610-b91b-82fb6a6ccdd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attention = AttentionNN(hidden_dim, num_heads)\n",
    "        self.ffn = FFN(hidden_dim)\n",
    "        self.norm1 = NormNN(hidden_dim)\n",
    "        self.norm2 = NormNN(hidden_dim)\n",
    "\n",
    "    def forward(self, X):\n",
    "        attention_output = self.attention(X)\n",
    "        X = X + attention_output\n",
    "\n",
    "        pog_X = self.norm1(X)\n",
    "\n",
    "        pogger_X = self.ffn(pog_X) + pog_X\n",
    "\n",
    "        poggers_X = self.norm2(pogger_X)\n",
    "\n",
    "        return poggers_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d4f0abb5-b123-4934-93a6-aa6b3d98582e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    def __init__(self,vocab_size , hidden_dim , num_heads,max_sequence_length, N):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = EmbeddingNN(vocab_size, hidden_dim, max_sequence_length)\n",
    "        self.blocks = nn.ModuleList([TransformerBlock(hidden_dim, num_heads) for i in range(N)])\n",
    "        self.linear = nn.Linear(hidden_dim,vocab_size)\n",
    "\n",
    "    def forward(self, X):\n",
    "        X = self.embedding(X)\n",
    "        for block in self.blocks:\n",
    "            X = block(X)\n",
    "        X = self.linear(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3411ebba-7f2a-479d-b16b-dc92ef20a7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "#integers = tokenizer.encode(text)\n",
    "#decode_dict = {i : chars for i , chars in enumerate(chars)}\n",
    "#encode_dict = {chars : i for i , chars in enumerate(chars)}\n",
    "\n",
    "#encode = lambda text : [encode_dict[i] for i in text]\n",
    "#decode = lambda indices : ''.join([decode_dict[i] for i in indices])\n",
    "\n",
    "#encoded_text = encode(text)\n",
    "\n",
    "#print(encode_dict['<END>'])\n",
    "#print(\"Max index in data:\", max(encoded_text), \"Vocab size:\", vocab_size)\n",
    " # usually 50256 for GPT-2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1cec522e-f44e-4802-ad93-2e5d2e5478bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =8\n",
    "hidden_dim = 768\n",
    "max_sequence_length = 512\n",
    "num_heads = 8\n",
    "seq_len = 128\n",
    "epochs = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a6356431-8af6-4192-9fd7-f5cdf6be7a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch Number: 1\n",
      "Epoch Number: 2\n",
      "Epoch Number: 3\n",
      "Epoch Number: 4\n",
      "Epoch Number: 5\n",
      "Epoch Number: 6\n",
      "Epoch Number: 7\n",
      "Epoch Number: 8\n",
      "Epoch Number: 9\n",
      "Epoch Number: 10\n",
      "Epoch Number: 11\n",
      "Epoch Number: 12\n",
      "Epoch Number: 13\n",
      "Epoch Number: 14\n",
      "Epoch Number: 15\n",
      "Epoch Number: 16\n",
      "Epoch Number: 17\n",
      "Epoch Number: 18\n",
      "Epoch Number: 19\n",
      "Epoch Number: 20\n",
      "Epoch Number: 21\n",
      "Epoch Number: 22\n",
      "Epoch Number: 23\n",
      "Epoch Number: 24\n",
      "Epoch Number: 25\n",
      "Epoch Number: 26\n",
      "Epoch Number: 27\n",
      "Epoch Number: 28\n",
      "Epoch Number: 29\n",
      "Epoch Number: 30\n",
      "Epoch Number: 31\n",
      "Epoch Number: 32\n",
      "Epoch Number: 33\n",
      "Epoch Number: 34\n",
      "Epoch Number: 35\n",
      "Epoch Number: 36\n",
      "Epoch Number: 37\n",
      "Epoch Number: 38\n",
      "Epoch Number: 39\n",
      "Epoch Number: 40\n",
      "Epoch Number: 41\n",
      "Epoch Number: 42\n",
      "Epoch Number: 43\n",
      "Epoch Number: 44\n",
      "Epoch Number: 45\n",
      "Epoch Number: 46\n",
      "Epoch Number: 47\n",
      "Epoch Number: 48\n",
      "Epoch Number: 49\n",
      "Epoch Number: 50\n",
      "Epoch Number: 51\n",
      "Epoch Number: 52\n",
      "Epoch Number: 53\n",
      "Epoch Number: 54\n",
      "Epoch Number: 55\n",
      "Epoch Number: 56\n",
      "Epoch Number: 57\n",
      "Epoch Number: 58\n",
      "Epoch Number: 59\n",
      "Epoch Number: 60\n",
      "Epoch Number: 61\n",
      "Epoch Number: 62\n",
      "Epoch Number: 63\n",
      "Epoch Number: 64\n",
      "Epoch Number: 65\n",
      "Epoch Number: 66\n",
      "Epoch Number: 67\n",
      "Epoch Number: 68\n",
      "Epoch Number: 69\n",
      "Epoch Number: 70\n",
      "Epoch Number: 71\n",
      "Epoch Number: 72\n",
      "Epoch Number: 73\n",
      "Epoch Number: 74\n",
      "Epoch Number: 75\n",
      "Epoch Number: 76\n",
      "Epoch Number: 77\n",
      "Epoch Number: 78\n",
      "Epoch Number: 79\n",
      "Epoch Number: 80\n",
      "Epoch Number: 81\n",
      "Epoch Number: 82\n",
      "Epoch Number: 83\n",
      "Epoch Number: 84\n",
      "Epoch Number: 85\n",
      "Epoch Number: 86\n",
      "Epoch Number: 87\n",
      "Epoch Number: 88\n",
      "Epoch Number: 89\n",
      "Epoch Number: 90\n",
      "Epoch Number: 91\n",
      "Epoch Number: 92\n",
      "Epoch Number: 93\n",
      "Epoch Number: 94\n",
      "Epoch Number: 95\n",
      "Epoch Number: 96\n",
      "Epoch Number: 97\n",
      "Epoch Number: 98\n",
      "Epoch Number: 99\n",
      "Epoch Number: 100\n",
      "Epoch Number: 101\n",
      "Epoch Number: 102\n",
      "Epoch Number: 103\n",
      "Epoch Number: 104\n",
      "Epoch Number: 105\n",
      "Epoch Number: 106\n",
      "Epoch Number: 107\n",
      "Epoch Number: 108\n",
      "Epoch Number: 109\n",
      "Epoch Number: 110\n",
      "Epoch Number: 111\n",
      "Epoch Number: 112\n",
      "Epoch Number: 113\n",
      "Epoch Number: 114\n",
      "Epoch Number: 115\n",
      "Epoch Number: 116\n",
      "Epoch Number: 117\n",
      "Epoch Number: 118\n",
      "Epoch Number: 119\n",
      "Epoch Number: 120\n",
      "Epoch Number: 121\n",
      "Epoch Number: 122\n",
      "Epoch Number: 123\n",
      "Epoch Number: 124\n",
      "Epoch Number: 125\n",
      "Epoch Number: 126\n",
      "Epoch Number: 127\n",
      "Epoch Number: 128\n",
      "Epoch Number: 129\n",
      "Epoch Number: 130\n",
      "Epoch Number: 131\n",
      "Epoch Number: 132\n",
      "Epoch Number: 133\n",
      "Epoch Number: 134\n",
      "Epoch Number: 135\n",
      "Epoch Number: 136\n",
      "Epoch Number: 137\n",
      "Epoch Number: 138\n",
      "Epoch Number: 139\n",
      "Epoch Number: 140\n",
      "Epoch Number: 141\n",
      "Epoch Number: 142\n",
      "Epoch Number: 143\n",
      "Epoch Number: 144\n",
      "Epoch Number: 145\n",
      "Epoch Number: 146\n",
      "Epoch Number: 147\n",
      "Epoch Number: 148\n",
      "Epoch Number: 149\n",
      "Epoch Number: 150\n",
      "Epoch Number: 151\n",
      "Epoch Number: 152\n",
      "Epoch Number: 153\n",
      "Epoch Number: 154\n",
      "Epoch Number: 155\n",
      "Epoch Number: 156\n",
      "Epoch Number: 157\n",
      "Epoch Number: 158\n",
      "Epoch Number: 159\n",
      "Epoch Number: 160\n",
      "Epoch Number: 161\n",
      "Epoch Number: 162\n",
      "Epoch Number: 163\n",
      "Epoch Number: 164\n",
      "Epoch Number: 165\n",
      "Epoch Number: 166\n",
      "Epoch Number: 167\n",
      "Epoch Number: 168\n",
      "Epoch Number: 169\n",
      "Epoch Number: 170\n",
      "Epoch Number: 171\n",
      "Epoch Number: 172\n",
      "Epoch Number: 173\n",
      "Epoch Number: 174\n",
      "Epoch Number: 175\n",
      "Epoch Number: 176\n",
      "Epoch Number: 177\n",
      "Epoch Number: 178\n",
      "Epoch Number: 179\n",
      "Epoch Number: 180\n",
      "Epoch Number: 181\n",
      "Epoch Number: 182\n",
      "Epoch Number: 183\n",
      "Epoch Number: 184\n",
      "Epoch Number: 185\n",
      "Epoch Number: 186\n",
      "Epoch Number: 187\n",
      "Epoch Number: 188\n",
      "Epoch Number: 189\n",
      "Epoch Number: 190\n",
      "Epoch Number: 191\n",
      "Epoch Number: 192\n",
      "Epoch Number: 193\n",
      "Epoch Number: 194\n",
      "Epoch Number: 195\n",
      "Epoch Number: 196\n",
      "Epoch Number: 197\n",
      "Epoch Number: 198\n",
      "Epoch Number: 199\n",
      "Epoch Number: 200\n",
      "Epoch Number: 201\n",
      "Epoch Number: 202\n",
      "Epoch Number: 203\n",
      "Epoch Number: 204\n",
      "Epoch Number: 205\n",
      "Epoch Number: 206\n",
      "Epoch Number: 207\n",
      "Epoch Number: 208\n",
      "Epoch Number: 209\n",
      "Epoch Number: 210\n",
      "Epoch Number: 211\n",
      "Epoch Number: 212\n",
      "Epoch Number: 213\n",
      "Epoch Number: 214\n",
      "Epoch Number: 215\n",
      "Epoch Number: 216\n",
      "Epoch Number: 217\n",
      "Epoch Number: 218\n",
      "Epoch Number: 219\n",
      "Epoch Number: 220\n",
      "Epoch Number: 221\n",
      "Epoch Number: 222\n",
      "Epoch Number: 223\n",
      "Epoch Number: 224\n",
      "Epoch Number: 225\n",
      "Epoch Number: 226\n",
      "Epoch Number: 227\n",
      "Epoch Number: 228\n",
      "Epoch Number: 229\n",
      "Epoch Number: 230\n",
      "Epoch Number: 231\n",
      "Epoch Number: 232\n",
      "Epoch Number: 233\n",
      "Epoch Number: 234\n",
      "Epoch Number: 235\n",
      "Epoch Number: 236\n",
      "Epoch Number: 237\n",
      "Epoch Number: 238\n",
      "Epoch Number: 239\n",
      "Epoch Number: 240\n",
      "Epoch Number: 241\n",
      "Epoch Number: 242\n",
      "Epoch Number: 243\n",
      "Epoch Number: 244\n",
      "Epoch Number: 245\n",
      "Epoch Number: 246\n",
      "Epoch Number: 247\n",
      "Epoch Number: 248\n",
      "Epoch Number: 249\n",
      "Epoch Number: 250\n",
      "Final loss: 9.416914690518752e-05\n"
     ]
    }
   ],
   "source": [
    "Transformer_model = Transformer(vocab_size, hidden_dim, num_heads, max_sequence_length, 12).to(device)\n",
    "optimizer = optim.Adam(Transformer_model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n = 0\n",
    "loss = None\n",
    "batch_y = []\n",
    "for epoch in range(epochs):\n",
    "    n = n+1\n",
    "    print(f\"Epoch Number: {n}\")\n",
    "    for start in range(0, len(data) - batch_size*seq_len, batch_size*seq_len):\n",
    "    \n",
    "        end = start+batch_size*seq_len\n",
    "        if end + 1 > len(data):\n",
    "            break\n",
    "    \n",
    "        batch_x = data[start : end].view(batch_size, seq_len).to(device)\n",
    "        batch_y = data[start+1 : end+1].view(batch_size, seq_len).to(device)\n",
    "        \n",
    "        logits = Transformer_model(batch_x).to(device)\n",
    "    \n",
    "        logits = logits.view(-1 , vocab_size)\n",
    "        batch_y = batch_y.view(-1)\n",
    "    \n",
    "        loss = criterion(logits, batch_y)\n",
    "    \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "if loss is not None:\n",
    "    print(\"Final loss:\", loss.item())\n",
    "else:\n",
    "    print(\"No batches were processed. Loss not defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3109a766-a275-494c-87f4-5b0697aa84ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(Transformer_model.state_dict(), \"Sigma_Transformer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb273179-342c-4f55-a528-fb433f410add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " hello rem\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " fulfilled make Tia-sama of us at between stories told are giving birth to our child….. are important.<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "text = input()\n",
    "input_ids = tokenizer.encode(text)\n",
    "input_text = torch.tensor([input_ids]).to(device)\n",
    "\n",
    "inference = []\n",
    "temperature = 0.8\n",
    "while True:\n",
    "    \n",
    "    logits = Transformer_model(input_text).to(device)\n",
    "    next_token = logits[:,-1 ,:]\n",
    "    probs = torch.softmax(next_token / temperature, dim=-1)\n",
    "    next_token_id = torch.multinomial(probs, 1)\n",
    "    input_text = torch.cat([input_text, next_token_id], dim=1)\n",
    "\n",
    "    inference.append(next_token_id.item())\n",
    "\n",
    "    if next_token_id.item() == 50256 or len(inference) >=  200:\n",
    "        break\n",
    "        \n",
    "print(tokenizer.decode(inference))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe18da-901d-4335-81a0-de653e17f57d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
